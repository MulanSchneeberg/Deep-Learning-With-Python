# GradientTape API
So far, TensorFlow seems to look a lot like NumPy, But here's something NumPy can not do: retrieve the gradient of any differentiable expression with respect to any of its inputs.
Just open a `GradientTape` scope, apply some computation to one or several input tensors, and reetrieve the gradient of the result with respect to the inputs.

````python
import tensorflow as tf
input_var=tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
  output=tf.square(input_var)
  gradient=tape.gradient(output, input_var)
print(gradient)

````

The GradientTape API is most commly used to retrieve the gradient of the loss of a model with respect to its weights. 

````python
gradient=tape.gradient(loss, weights)
````

So far, you’ve only seen the case where the input tensors in tape.gradient() were TensorFlow variables. It’s actually possible for these inputs to be any arbitrary tensor. However, only trainable variables are tracked by default. With a constant tensor, you’d
have to manually mark it as being tracked by calling ``tape.watch()`` on it.

````python
input_const=tf.constant(.3)
with tf.GradientTape() as tape:
    tape.watch(input_const)
    output=tf.square(input_const)
    gradient=tape.gradient(output, input_const)
print(gradient)
````
Why is this necessary? Because it would be too expensive to preemptively store the information required to compute the gradient of anything with respect to anything. To avoid wasting resources, the tape needs to know what to watch. Trainable variables
are watched by default because computing the gradient of a loss with regard to a list of trainable variables is the most common use of the gradient tape.

## Second-prder gradients

````python
time=tf.Variable(0.)
with tf.GradientTape() as outer_tape:
  with tf.GradientTape() as inner_tape:
    position=4.9*time**2
    speed=inner_tape.gradient(position, time)
  speed=outer_tape.gradient(speed, time)
print(speed)
  
````