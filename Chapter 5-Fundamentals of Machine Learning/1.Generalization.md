# Generalization: The goal of machine learning

The fundamental issue in machine learning is the tension between optimization and generalization. Optimization refers to the process of adjusting a model to get the best performance possible on the training data (the learning in machine learning),whereas generalization refers to how well the trained model performs on data it has never seen before. The goal of the game is to get good generalization, of course, but you don’t control generalization; you can only fit the model to its training data. If you do that
too well, overfitting kicks in and generalization suffers.

## Overfitting is universal

![](./screenshots/overfit.jpg)

**Overfitting is particularly likely to occur when your data is noisy, if it involves uncertainty, or if it includes rare features.**

### Noisy Training data

![](./screenshots/noisyData.jpg)

### Ambiguous features

![](./screenshots/AmbiguousFeatures.jpg)

### Rare features and spurious correlations

![](./screenshots/spuriousCorrelation.jpg)
Noisy features inevitably lead to overfitting. As such, in cases where you aren’t sure whether the features you have are informative or distracting, it’s common to do feature selection before training. Restricting the IMDB data to the top 10,000 most common
words was a crude form of feature selection, for instance. The typical way to do feature selection is to compute some usefulness score for each feature available—a measure of how informative the feature is with respect to the task, such as the mutual information between the feature and the labels—and only keep features that are above some threshold. Doing this would filter out the white noise channels in the preceding example.